{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPu0hkkTK7BAoeJDMc0Hr1I"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Nri4MWMP1BM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e67f8e75-d217-4a3d-a8a1-99cf524265e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyautogen\n",
            "  Downloading pyautogen-0.2.0-py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.4/114.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from pyautogen) (5.6.3)\n",
            "Collecting flaml (from pyautogen)\n",
            "  Downloading FLAML-2.1.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.2/295.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai~=1.2 (from pyautogen)\n",
            "  Downloading openai-1.3.6-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv (from pyautogen)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from pyautogen) (2.3.0)\n",
            "Collecting tiktoken (from pyautogen)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.2->pyautogen) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai~=1.2->pyautogen) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai~=1.2->pyautogen)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.2->pyautogen) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai~=1.2->pyautogen) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai~=1.2->pyautogen) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai~=1.2->pyautogen) (4.5.0)\n",
            "Requirement already satisfied: NumPy>=1.17.0rc1 in /usr/local/lib/python3.10/dist-packages (from flaml->pyautogen) (1.23.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->pyautogen) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->pyautogen) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai~=1.2->pyautogen) (3.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai~=1.2->pyautogen) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai~=1.2->pyautogen) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai~=1.2->pyautogen)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.2->pyautogen)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->pyautogen) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->pyautogen) (2.0.7)\n",
            "Installing collected packages: python-dotenv, h11, flaml, tiktoken, httpcore, httpx, openai, pyautogen\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flaml-2.1.1 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.6 pyautogen-0.2.0 python-dotenv-1.0.0 tiktoken-0.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyautogen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
        "config_list =  [{'model': 'gpt-3.5-turbo', 'api_key': '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'},]\n",
        "llm_config = {\n",
        "    \"seed\":47,\n",
        "    \"config_list\": config_list,\n",
        "    \"temperature\":0\n",
        "}\n"
      ],
      "metadata": {
        "id": "OHIUG7cq1NnW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = AssistantAgent(\"assistant\",\n",
        "                           is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
        "                           llm_config=llm_config,\n",
        "                           system_message = \"Reply TERMINATE when the task has been solved\"\n",
        "                            )\n",
        "\n",
        "user_proxy = UserProxyAgent(\n",
        "    \"user_proxy\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    is_termination_msg=lambda x: True if \"I can help you with?\" in x.get(\"content\") else False,\n",
        "    max_consecutive_auto_reply= 10,\n",
        "    code_execution_config={\"work_dir\": \"coding\", \"use_docker\":False},\n",
        "    system_message = \"Reply TERMINATE if the task has been solved for your full satisfaction. Or reply CONTINUE if the task is not solved yet\",\n",
        "\n",
        "    )\n",
        "\n",
        "#user_proxy.initiate_chat(assistant, message=\"What date is today?\")"
      ],
      "metadata": {
        "id": "ipOaw3Su9kyg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task1 = \"Create a program that reads the file result.txt\"\n",
        "user_proxy.initiate_chat(assistant,message = task1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8jbUDP1-TMj",
        "outputId": "5e1afc0b-8d95-446f-fee7-0404ee89e076"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to assistant):\n",
            "\n",
            "Create a program that reads the file result.txt\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "Sure! Here's a Python program that reads the contents of a file named \"result.txt\":\n",
            "\n",
            "```python\n",
            "# Open the file in read mode\n",
            "file = open(\"result.txt\", \"r\")\n",
            "\n",
            "# Read the contents of the file\n",
            "contents = file.read()\n",
            "\n",
            "# Print the contents\n",
            "print(contents)\n",
            "\n",
            "# Close the file\n",
            "file.close()\n",
            "```\n",
            "\n",
            "Save this code in a Python file, run it, and it will read the contents of the \"result.txt\" file and print them to the console.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\n",
            "user_proxy (to assistant):\n",
            "\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: \n",
            "1000\n",
            "2000\n",
            "3000\n",
            "\n",
            "4000\n",
            "\n",
            "5000\n",
            "6000\n",
            "\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "\n",
            "10000\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "Great! It seems like the program successfully read the contents of the \"result.txt\" file and printed them to the console. The contents of the file are:\n",
            "\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "\n",
            "4000\n",
            "\n",
            "5000\n",
            "6000\n",
            "\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "\n",
            "10000\n",
            "\n",
            "If you have any more questions or need further assistance, feel free to ask!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "user_proxy (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "Is there anything else I can help you with?\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task2 = \"Sum the groups in the file and return the largest sum\"\n",
        "user_proxy.initiate_chat(assistant,message = task2, clear_history=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG4g0kuBXTym",
        "outputId": "82df14b2-8172-4a60-99cb-f6cb004b7240"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to assistant):\n",
            "\n",
            "Sum the groups in the file and return the largest sum\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "Certainly! Here's an updated version of the program that reads the file \"result.txt\", calculates the sum of each group of numbers, and returns the largest sum:\n",
            "\n",
            "```python\n",
            "# Open the file in read mode\n",
            "file = open(\"result.txt\", \"r\")\n",
            "\n",
            "# Read the contents of the file\n",
            "contents = file.read()\n",
            "\n",
            "# Split the contents into groups of numbers\n",
            "groups = contents.split(\"\\n\\n\")\n",
            "\n",
            "# Initialize variables\n",
            "largest_sum = 0\n",
            "\n",
            "# Iterate over each group\n",
            "for group in groups:\n",
            "    # Split the group into individual numbers\n",
            "    numbers = group.split(\"\\n\")\n",
            "    \n",
            "    # Remove any empty strings from the list of numbers\n",
            "    numbers = [num for num in numbers if num]\n",
            "    \n",
            "    # Calculate the sum of the numbers in the group\n",
            "    group_sum = sum(int(num) for num in numbers)\n",
            "    \n",
            "    # Update the largest sum if necessary\n",
            "    if group_sum > largest_sum:\n",
            "        largest_sum = group_sum\n",
            "\n",
            "# Print the largest sum\n",
            "print(\"The largest sum is:\", largest_sum)\n",
            "\n",
            "# Close the file\n",
            "file.close()\n",
            "```\n",
            "\n",
            "Save this code in a Python file, run it, and it will read the contents of the \"result.txt\" file, calculate the sum of each group of numbers, and return the largest sum. In this case, the largest sum is 74394.\n",
            "\n",
            "Let me know if you need any further assistance!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\n",
            "user_proxy (to assistant):\n",
            "\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: \n",
            "The largest sum is: 74394\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "Great! It seems like the program successfully calculated the sum of each group of numbers in the \"result.txt\" file and returned the largest sum, which in this case is 74394.\n",
            "\n",
            "If you have any more questions or need further assistance, feel free to ask!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "user_proxy (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "Is there anything else I can help you with?\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"Find and return the 3 largest sums. What is the sum of them\"\n",
        "user_proxy.initiate_chat(assistant,message = task, clear_history=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8FU14Mp-150",
        "outputId": "a4c4387f-76e6-4457-d48f-17218cabf12c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to assistant):\n",
            "\n",
            "Find and return the 3 largest sums. What is the sum of them\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "Certainly! Here's an updated version of the program that reads the file \"result.txt\", calculates the sum of each group of numbers, finds the three largest sums, and returns the sum of those three largest sums:\n",
            "\n",
            "```python\n",
            "# Open the file in read mode\n",
            "file = open(\"result.txt\", \"r\")\n",
            "\n",
            "# Read the contents of the file\n",
            "contents = file.read()\n",
            "\n",
            "# Split the contents into groups of numbers\n",
            "groups = contents.split(\"\\n\\n\")\n",
            "\n",
            "# Initialize variables\n",
            "largest_sums = []\n",
            "num_largest_sums = 3\n",
            "\n",
            "# Iterate over each group\n",
            "for group in groups:\n",
            "    # Split the group into individual numbers\n",
            "    numbers = group.split(\"\\n\")\n",
            "    \n",
            "    # Remove any empty strings from the list of numbers\n",
            "    numbers = [num for num in numbers if num]\n",
            "    \n",
            "    # Calculate the sum of the numbers in the group\n",
            "    group_sum = sum(int(num) for num in numbers)\n",
            "    \n",
            "    # Update the list of largest sums\n",
            "    if len(largest_sums) < num_largest_sums:\n",
            "        largest_sums.append(group_sum)\n",
            "    else:\n",
            "        min_sum = min(largest_sums)\n",
            "        if group_sum > min_sum:\n",
            "            largest_sums.remove(min_sum)\n",
            "            largest_sums.append(group_sum)\n",
            "\n",
            "# Calculate the sum of the three largest sums\n",
            "sum_of_largest_sums = sum(largest_sums)\n",
            "\n",
            "# Print the sum of the three largest sums\n",
            "print(\"The sum of the three largest sums is:\", sum_of_largest_sums)\n",
            "\n",
            "# Close the file\n",
            "file.close()\n",
            "```\n",
            "\n",
            "Save this code in a Python file, run it, and it will read the contents of the \"result.txt\" file, calculate the sum of each group of numbers, find the three largest sums, and return the sum of those three largest sums. \n",
            "\n",
            "Let me know if you need any further assistance!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\n",
            "user_proxy (to assistant):\n",
            "\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: \n",
            "The sum of the three largest sums is: 212836\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "Great! It seems like the program successfully calculated the sum of the three largest sums in the \"result.txt\" file, which in this case is 212836.\n",
            "\n",
            "If you have any more questions or need further assistance, feel free to ask!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "user_proxy (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "Is there anything else I can help you with?\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}